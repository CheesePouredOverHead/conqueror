# 选题描述
题目规则（简化版）如下：
1、游戏共有10种不同的卡牌，分别编号0-9。
2、游戏设置卡牌区和暂存区。
3、卡牌区中的牌随机发放，不同层行列错开半张牌。最上层的牌可以移动到暂存区，没有被全部遮挡的牌可以知道标号但不可移动，剩余牌不可知也不可移动。
4、暂存区的卡牌三个可以消除。卡牌区容量有限，存满则游戏失败，存满前将卡牌区的牌全部消除则游戏成功。

# 方案设计
使用面向对象编程搭建环境。
ai方案有三种：赋分算法（我也不知道怎么取名）、DQN和PPO。
从效果来说，赋分>PPO>>DQN
虽然PPO玩得也不算太好，但至少有选择与暂存区牌同号牌的意识。DQN……写都写了，放上来吧。赋分算法还是比较强的！！虽然慢。
## 赋分算法
主体采用贪心算法，即为每种可能选择进行打分，最后选择得分最高的一张牌进行移动。
单张牌的评分规则如下：
- 在暂存区空位不止一个的情况下：
    - 暂存区已有两张同号牌，选择即可消除，得10分
    - 卡牌区能点的同号牌不少于3张，得6分
    - 到下一步一定可以消除，得4分
    - 其余情况0分
- 暂存区只有一个空位：（保命原则）
    - 马上消除得10分
    - 其余情况0分
但是仅仅依靠但张牌赋分选择表现不佳，于是我设计了综合后三步的赋分plus算法：
- 每一步的得分是这一步得分乘以权重，加上下一步得分的所有情况的最大值
- 采用递归实现，步数==3或暂存区满则返回
- 因为如果每步权重相同，则可能出现“方案A第三步能消，分数最高，所以此时选择牌a；但是下一轮赋分时方案B得分更高”这种朝令夕改最后什么都消不掉的情况，所以设置第一步权重1，第二步权重0.7，第三步权重0.5，鼓励马上消除

这种算法的实战表现非常好，基本上可以破解各种场景，但由于计算量大，在牌多的时候算得慢。
## DQN算法
Q-Learning是一种基于值的强化学习算法，目的是找到一个策略，使得在给定状态下的期望累积奖励最大化。Q值函数Q(s, a)表示在状态s采取动作a时的期望累积奖励。
深度Q-学习（Deep Q-learning）是一个利用深度卷积神经网络来进行Q-学习的算法。DQN通过使用经历回放，也就是每次学习的时候并不直接从最近的经历中学习，而是从之前的经历中随机采样来进行训练。

DQN算法步骤：

1. 初始化经验回放记忆池
2. 初始化行为Q网络和目标Q网络，参数相同
3. 重复以下步骤（对于每一轮训练）：
   1. 根据当前状态选择动作（ε-贪婪策略）
   2. 执行动作，观察奖励和新状态
   3. 将经验（状态，动作，奖励，新状态）存入记忆池
   4. 从记忆池中随机采样一个小批量的经验
   5. 计算目标Q值： 
      \[
      y = \begin{cases} 
      r & \text{如果终止状态} \\ 
      r + \gamma \max_{a'} Q'(s', a') & \text{如果非终止状态}
      \end{cases}
      \]
   6. 更新行为Q网络，最小化均方误差：
      \[
      L = \mathbb{E}[(y - Q(s, a))^2]
      \]
   7. 每隔C步，将行为Q网络的参数复制到目标网络
   
代码参考https://hrl.boyuai.com/chapter/2/dqn%E7%AE%97%E6%B3%95/

## PPO算法
PPO（Proximal Policy Optimization）是一种改进的策略优化算法，用于提高强化学习中的样本效率和训练稳定性。
步骤如下：
1. 收集样本：
   通过与环境交互，使用当前策略 \( \pi_\theta \) 收集一批状态、动作、奖励和下一状态的数据。

2. 计算优势函数：
   使用优势函数估计方法（如GAE）计算每个时间步的优势值 \( \hat{A}_t \)。
   \[
   \hat{A}_t = \delta_t + (\gamma \lambda) \delta_{t+1} + (\gamma \lambda)^2 \delta_{t+2} + \dots
   \]
   其中 \( \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) \)。

3. 计算损失函数：
   计算剪切损失函数 \( L^{CLIP}(\theta) \)。在每次更新中，通过限制策略变化的比例来确保更新的稳定性。
   \[
   L^{CLIP}(\theta) = \mathbb{E} \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]
   \]
   其中 \( r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} \)。

4. 策略优化：
   使用随机梯度上升方法优化损失函数。一般会执行多次小步更新来替代一次大步更新，以进一步提高稳定性。

5. 策略更新：
   用优化后的参数更新策略，并重复上述步骤。

在具体实现中，训练使用了[DI-engine](https://github.com/opendilab/DI-engine?tab=readme-ov-file)的PPO Policy

# 代码模块的功能划分与描述
结构概览：
```text
基础实现：
ai.py ->AI类，相当于一个agent
card.py ->Card类
stack.py ->Stack类（暂存区类）和Pile类（卡牌区类）
generate.py ->生成牌局
main.py ->进入游戏的入口
ui.py ->ui界面

gym环境:
sheepenv.py
sheepenv2.py
sheepenv4.py
sheepenv5.py
sheepenv6.py

dqn训练：
dqn.py
dqn2.py
dqn3.py
dqn4.py ->最终版
rl_utils.py ->训练依赖的一个包，从《动手学强化学习》下的
model_with_env6.pth ->储存的训练好的模型

ppo训练：
ppo.py __
ppo2.py _|->进行训练的代码
ppo3.py _|
sheep_model.py __
model2.py _______|->模型（agent）的类
sheep_ppo_env6_240601_111849 ->文件夹，储存ppo训练的模型和日志

```
==最后投入使用的是sheepenv6,dqn4,ppo3,model2，其他介绍在“迭代过程”中。由于历史版本的函数其实大同小异，所以函数文档统一在最终版呈现。==

## `card.py`
### Card 类（卡片类）
- `self.no`:该牌号码
- `self.x` `self.y`:该牌坐标
- `self.floor`:该牌最开始所在层数
- `self.district`:该牌所在区（卡牌区/暂存区）
- `self.up`:一个列表，列表内是所有在这张牌上方的牌
- `self.below`:一个列表，列表内是所有在这张牌下方的牌
- `self.btn`:一个按钮，图形化界面中的该卡牌
- display(self):在图形页面中展示这张牌。
- can_see(self):判断这张牌是否可见。原理是把self.up中的所有牌的btn取并集，判断self.btn是否被包含于并集中。使用PyQT中的相关函数实现。
- move(self,index=None):移动该牌。若从卡牌区移至暂存区，不必传入index。若因暂存区消除导致移动，需要传入消除后在暂存区的序号index。若从卡牌区移至暂存区，先后调用stack.add和pile.move，再从below列表中的卡牌的up列表中移除自己。

## `stack.py`
### Stack 类（暂存区类）
- `self.capacity`:暂存区容量
- `self.inside`:目前暂存区中的卡牌数
- `self.dic`:字典，键为0-9，值为列表，列表内为号码为键的卡牌在暂存区的序号
- `self.lst`:列表，列表内为所有暂存区的卡牌
- get_location(self,index=None):传回卡片移动后的坐标
- add(self,card):往暂存区添加卡片
- judge(self):判断是否因暂存区满导致失败
- eliminate(self,to_delete):消除暂存区中相同的三个牌，更新其它牌在stack.lst中的编号。to_delete是包含待消除的牌在stack.lst（暂存区所有牌的列表）中的序号的列表。
### Pile 类（卡牌区类）
- `self.inside`:卡牌区中剩余牌数
- `self.setting`:卡牌区设定总牌数
- `self.lst`:卡牌区所有牌列表
- `self.floor`:卡牌区层数
- move(self,card):把某卡牌从卡牌区移走，lst中相应位置更新为None。
- judge(self):判断是否因为卡牌区牌全部移走而获胜
- detect(self):遍历lst中所有卡牌，判断卡牌两两是否相交，在下面牌的up，上面牌的below增加彼此。判断相交由PyQT中的intersects函数实现。

## `ui.py`
### Setting类
设定暂存区容量、层数、每层牌数、模式的ui界面，根据输入和选择的结果设定参数。
### Play类
- play(self):开始游戏
- win(self):胜利后的ui界面
- lose(self):失败后的ui界面

## `ai.py`
### AI类
- `self.work`:是否开启自动模式
- `self.on_going`:ai是否继续运行（游戏结束则为False）
- __init__(self)：一些参数设置，加载模型。self.model1是DQN的模型，self.model2是PPO的模型。

- greedy(self):使用贪心算法进行游戏,每次移动得分最高的牌
- score(self,card,canmove,cansee,dicc):card为被评估的牌，canmove为可移动牌的列表，cansee为能看见的牌的列表，dicc为此时暂存区的dic。每一步的价值评估函数。
- score2(self,step,card,canmove,cansee,dicc):card为被评估的牌，step是现在这个递归看到后几步，canmove为可移动牌的列表，cansee为能看见的牌的列表，dicc为此时暂存区的dic。往后看三步的价值评估函数。
具体的计算方法见方案设计。

- get_obs5(self):将现在的情况化成相应的env6的observation（np数组）。
- dqn_work5(self):当游戏没结束，调用get_obs5获得observation，传入model1（dqn，env6的模型）获得相应的action，移动`pile.lst[action]`。
- ppo.work(self):基本同上，传入model2(ppo,env6的模型)。

## `generate.py`
- generate(w):w是显示窗口。该函数生成本局游戏的所有卡牌
- all_card():生成所有牌的序号，保证每种牌的数量都是3的倍数

## `main.py`
- main:创建一个QApplication，然后依次打开Setting和Play的ui界面

## `sheepenv6.py`
SheepEnv类继承自gym.Env,具有__init__,step,reset三个接口，观察空间observation_space，动作空间action_space。因为每个环境的__init__和reset大同小异，均为生成新牌局，约等于all_card+generate，action_space均为0-179的离散空间（设置卡牌数上限180），所以在本环境以及后续环境中不加赘述，重点介绍每个函数的observation，和step的奖励设置。

### observation
```python
self.observation_space = spaces.Dict({
            "card_num": spaces.Box(low=0,high=1,shape=(180,11), dtype=np.float64),
            "movable_cards": spaces.Box(low=0,high=1,shape=(180,), dtype=np.float64),
            "relation":spaces.Box(low=-1,high=1,shape=(180,180), dtype=np.float64),
            "stack_positions": spaces.Box(low=0, high=1, shape=(30,), dtype=np.float64)
        })
```
- card_num:(180,11),obs['card_num'][i][第i张牌的值]=1，没牌则obs['card_num'][i][10]=1
- movable_cards:（180，），第i张牌能动为1，不能动为0.
- relation：(180,180),若第i张牌在第j张牌上面，obs['relation'][i][j]=1，obs['relation'][j][i]=-1,不重叠为0.如果第i张牌被移走，相应行列都为0.
- stack_positions:(30,),stack中不同值分别有几张。例如有1张0则obs['stack_positions'][1]=1

| 暂存区有几张0 | … | 暂存区有几张9   |
|---------|---|-----------|
| [0]-[2] |   | [27]-[29] |
| 0/1     |   |0/1|

### step
基础奖励为赋分算法的综合后三步赋分（在算法介绍中），在此基础上赢了+50，输了-50.

## `dqn4.py`
前面的dqn也类似，但是是针对不同的环境的设计，此处略去不表。最后使用的是dqn4的结果。

### DQN类
继承自nn.Module，有三层神经网络。
- forward：向前传播

### Agent类
- __init__(self,state_size,action_size):设置学习过程相关参数
- remember(self, state, action, reward, next_state, done):存入记忆池中
- choose_action(self, state)：根据现在的状态，从movable_cards中拉取合法动作列表，若随机数小于等于epsilon则随机生成动作（探索），否则将状态投入神经网络得到每个动作评分，将非法动作的得分赋为-inf，返回得分最高的动作
- replay(self, batch_size):从记忆中随机抽取一批经验进行训练。

训练过程就是设定训练总数，然后循环生成新游戏让它学习。

## `model2.py`
参考了[这个模型](https://github.com/opendilab/DI-sheep)
### ItemEncoder类
继承自nn.Module,有__init__,forward两个函数。
有多种编码模式，处理比较复杂的状态（card_num和relation）

### SheepModel
继承自nn.Module.
Attributes:
    num_encoder (ItemEncoder): card_num编码器。
    stack_encoder (MLP): stack编码器。
    relation_encoder (ItemEncoder): relation编码器。

Methods:
    compute_actor: 计算行为。
    compute_critic: 计算评估值。
    compute_actor_critic: 计算行为和评估值。
    forward: 前向传播。
    compute_action: 计算最佳动作。

## `ppo3.py`
- sheep_ppo_config:参数字典
- main：创建一个model（来自model2），learner,collector,evaluator和训练的ppo policy来自DI-engine。重复生成新的训练直到达到步数上限或者训练成果（平均reward）超过停止值。

# 迭代过程
## `sheepenv.py`

### observation
```python
self.observation_space = spaces.Dict({
            "card_num": spaces.Box(low=-1,high=9,shape=(180,), dtype=np.int32),
            "movable_cards": spaces.Box(low=0,high=1,shape=(180,), dtype=np.int32),
            "relation":spaces.Box(low=-1,high=1,shape=(180,180), dtype=np.int32),
            "stack_positions": spaces.Box(low=-1, high=9, shape=(7,), dtype=np.int32)
        })
```
这个observation是一个字典，一共有四个键：card_num,movable_cards,relation,stack_positions.所有类型都为np.int32

  - card_num：记录每张牌的值，(180,)，第i个位的值为第i张牌的值，没牌就是-1.
  - movable_cards:（180，），第i张牌能动为1，不能动为0.
  - relation：(180,180),若第i张牌在第j张牌上面，obs['relation'][i][j]=1，obs['relation'][j][i]=-1,不重叠为0.如果第i张牌被移走，相应行列都为0.
  - stack_position:(7,)默认暂存区容量7,stack中第i张牌的值，没牌则为-1.

### step(self,action)
移动第action张牌。如果action实际上不能移动，reward=-1000。如果成功消除，基础奖励为50*0.5*(总牌数//3)，如果游戏结束，赢了reward=基础奖励+50，输了reward=基础奖励-50，没结束reward=基础奖励。

但这个env训练效果并不好，我推测是因为observation太乱了，于是写了第二版。

## `sheepenv2.py`
### observation
```python
self.observation_space = spaces.Dict({
            "pile": spaces.Box(low=-1,high=375,shape=(180,5), dtype=np.int32),
            "stack": spaces.Box(low=-1, high=9, shape=(7,), dtype=np.int32)
        })
```
简化成了只有两个键的字典，stack和上面的stack_positions一样，pile即卡牌堆的矩阵内涵如表所示。类型np.int32
|第i张牌|牌号|	第几层	|x坐标|	y坐标	|能不能动|
|-|-|-|-|-|-|
|索引|	[0]|	[1]|	[2]|	[3]|	[4]|
|0|	1|	1|	100|	40|	1|
|1||||||								
|…||||||					
|179||||||					

### step
赢了500，输了-500，非法动作-1000.没赢没输：基础奖励：卡牌区减少的牌数/总数*500；成功消除再加300.

此时训练效果依然不好，于是有了sheepenv4。

## `sheepenv4.py`
因为前两个效果都不好，于是上网找参考，参考了[这个环境](https://github.com/opendilab/DI-sheep)。
我破译了一下这个环境，发现了两点细节上的不同：
1、这个环境的数据类型都是np.float32
2、矩阵中没有非0，1的数
所以我推测这两点在强化学习的矩阵运算中比较有影响，导致我之前的训练效果较差。

于是我进行了一些本土化改编，得到了env4.

### observation：
```python
self.observation_space = gym.spaces.Dict(
            {
                'item_obs': gym.spaces.Box(0, 1, dtype=np.float32, shape=(180, 39)),
                'stack_obs': gym.spaces.Box(0, 1, dtype=np.float32, shape=(30, )),
                'global_obs': gym.spaces.Box(0, 1, dtype=np.float32, shape=(self.global_size, )),
                'action_mask': gym.spaces.Box(0, 1, dtype=np.float32, shape=(180, )) 
            }
        )
```
是一个含有4个键的字典。

#### item_obs

| 第i张牌 | 牌号  | 能不能动 | 有没有被移走 | x坐标（在第几列）（13列） | y坐标（在第几行）（13行） | 可见否  |
|------|-----|------|--------|----------------|----------------|------|
| 索引   |  [0]-[9]   | [10] | [11]   | [12]-[24]      | [25]-[37]      | [38] |
| 0    | 1/0 | 1    | 0      | 1/0            | 1/0            | 1    |
| …    |     |      |        |                |                |      |
| 179  |     |      |        |                |                |      |

这是关于卡牌区中牌的信息。
在相应位置1，其余0.
卡牌区设置是7*7的大小，但是错开半张牌放置，所以每张卡牌的x（y）坐标有13种可能

#### stack_obs

| 暂存区有几张0 | … | 暂存区有几张9   |
|---------|---|-----------|
| [0]-[2] |   | [27]-[29] |
| 0/1     |   |0/1|

暂存区牌的信息。举例：若有2张1，则observation['stack_obs'][5]=1

#### global_obs
前十个位置表示还剩百分之多少（一共30张，还剩12张，则[3]的位置为1其余为0.后7个位置表示暂存区有几张牌。大概是对全局情况的统览。

#### action_mask
=env1的movable_cards，表示哪个能动

### step
无功无过0，消了30，赢了+50，输了-50.

效果依然不理想，猜想是因为item_obs很乱+基础规则奖励在小规模训练中可能无法取得好的效果。于是综合env1，env4和赋分算法写了env5.

## `sheepenv5.py`
observation和env6一模一样，区别就在step的基础奖励设置，遵照单步赋分的规则而非综合三步。

# 功能展示
## 手动模式
![1717329604875.gif](https://s2.loli.net/2024/06/02/rtqzBPI85ELTwaS.gif)
## 赋分算法
![1717330438687.gif](https://s2.loli.net/2024/06/02/swhcYlRJzPQADGI.gif)
## DQN
![1717330769188.gif](https://s2.loli.net/2024/06/02/U1QCl4tSk3VyHnN.gif)
## PPO
![1717330138187.gif](https://s2.loli.net/2024/06/02/Vg8WBmDK3roSkAh.gif)
